# Vector DB Benchmarking - Project State

**Last Updated**: 2025-11-03
**Status**: Phase 2 Complete - All 7 Databases Benchmarked and Verified âœ…

---

## ğŸ¯ What We've Accomplished

### Phase 1 & 2: Complete Benchmarking Framework for All 7 Vector Databases âœ…

We have built a comprehensive benchmarking framework with:

1. **All 7 Vector Database Adapters** âœ…
   - FAISS (embedded, in-memory)
   - Chroma (embedded/server)
   - Qdrant (client-server)
   - pgvector (PostgreSQL extension)
   - Weaviate (GraphQL API)
   - Milvus (distributed)
   - OpenSearch (k-NN plugin)

2. **Query Performance Benchmarks** (for all databases)
   - Measures query latency across different top-K values (1, 3, 5, 10, 20)
   - Tracks throughput (QPS)
   - Automated quality metrics using semantic similarity
   - Generates 4-panel visualization (speed, throughput, quality, tradeoffs)
   - Individual scripts for each database: `Scripts/run_*_benchmark.py`

3. **Ingestion Performance Benchmarks**
   - Tests chunk sizes (256, 512, 1024 characters)
   - Tests batch sizes (50, 100 docs/batch)
   - Tests document scaling (10, 20 docs)
   - Individual scripts: `Scripts/run_*_ingestion_benchmark.py`

4. **Cross-Database Comparison**
   - `Scripts/create_comparison.py` generates unified comparison plots
   - Shows all 7 databases on same axes for fair comparison

5. **Verified and Validated**
   - Fixed critical bugs in FAISS (L2 distance conversion) and OpenSearch (score normalization)
   - All similarity calculations verified
   - See [BENCHMARK_VERIFICATION.md](BENCHMARK_VERIFICATION.md) for validation details

6. **Test Infrastructure**
   - 20 climate science documents in `Data/test_corpus/documents/`
   - 10 test queries with ground truth in `Data/test_corpus/test_cases.json`
   - Docker Compose setup for all 7 vector databases
   - **Next**: Integrate curated dataset from Google Drive for comprehensive testing

---

## ğŸ“Š Latest Results (All 7 Databases)

### Query Performance & Quality Comparison @ K=5

| Database   | Latency (ms) | Throughput (QPS) | Top-1 Quality | Avg Quality | Trend |
|------------|--------------|------------------|---------------|-------------|-------|
| **FAISS**      | 3.96 | **252.6** ğŸ† | 0.656 | 0.605 | âœ… Decreasing |
| **Chroma**     | 4.53 | 220.6 | **0.732** ğŸ† | 0.666 | âœ… Decreasing |
| **pgvector**   | 7.54 | 132.6 | 0.732 | 0.666 | âœ… Decreasing |
| **Qdrant**     | 7.87 | 127.1 | 0.732 | 0.666 | âœ… Decreasing |
| **Weaviate**   | 9.83 | 101.7 | 0.732 | 0.666 | âœ… Decreasing |
| **Milvus**     | 10.31 | 97.0 | 0.732 | 0.666 | âœ… Decreasing |
| **OpenSearch** | 12.35 | 81.0 | 0.732 | 0.666 | âœ… Decreasing |

**Key Insights**:
- **Fastest**: FAISS (252.6 QPS) - in-memory, no network overhead
- **Best Quality**: 6-way tie at 0.732 (all cosine similarity databases)
- **FAISS Quality**: Slightly lower (0.656) due to L2 distance metric vs cosine similarity
- **All databases show correct decreasing similarity trends** with increasing K
- **Verification**: All results validated in [BENCHMARK_VERIFICATION.md](BENCHMARK_VERIFICATION.md)

### Qdrant Detailed Results (Example)

**Query Performance**:
```
Top-K    Avg (ms)     P95 (ms)     QPS        Avg Sim    Top-1 Sim
----------------------------------------------------------------------
1        26.76        66.22        37.37      0.732      0.732
3        8.13         10.61        123.01     0.688      0.732
5        7.87         8.91         127.12     0.666      0.732
10       9.67         12.45        103.43     0.629      0.732
20       10.59        11.45        94.47      0.574      0.732
```

**Ingestion Performance**:
- Ingestion time: 0.51s for 175 chunks
- ~340 chunks/second throughput
- Minimal variance across chunk sizes

---

## ğŸ”§ Technical Implementation

### Quality Metrics (NEW Feature)

**Automated semantic similarity measurement**:
- Uses cosine similarity between query and retrieved chunk embeddings
- No manual labeling required
- Provides objective, continuous quality scores (0-1 scale)

**Metrics tracked**:
- `avg_similarity`: Mean similarity across all top-K results
- `avg_top1_similarity`: Quality of best result
- `min_similarity`: Quality of worst result

**Why this approach**:
- âœ… Fast and automated (speed-to-data)
- âœ… Objective and reproducible
- âœ… Foundation for contributors to add more sophisticated metrics later

### File Structure
```
Scripts/
â”œâ”€â”€ run_faiss_benchmark.py            # FAISS (embedded)
â”œâ”€â”€ run_chroma_benchmark.py           # Chroma (embedded/server)
â”œâ”€â”€ run_qdrant_benchmark.py           # Qdrant (client-server)
â”œâ”€â”€ run_pgvector_benchmark.py         # PostgreSQL + pgvector
â”œâ”€â”€ run_weaviate_benchmark.py         # Weaviate (GraphQL)
â”œâ”€â”€ run_milvus_benchmark.py           # Milvus (distributed)
â”œâ”€â”€ run_opensearch_benchmark.py       # OpenSearch (k-NN)
â”œâ”€â”€ run_*_ingestion_benchmark.py      # Ingestion benchmarks
â””â”€â”€ create_comparison.py              # Cross-database comparison

src/
â”œâ”€â”€ vector_dbs/
â”‚   â”œâ”€â”€ base_benchmark.py             # Abstract base class
â”‚   â”œâ”€â”€ rag_benchmark.py              # RAG-specific base
â”‚   â”œâ”€â”€ faiss_adapter.py              # FAISS implementation (L2 distance fix applied)
â”‚   â”œâ”€â”€ chroma_adapter.py             # Chroma implementation
â”‚   â”œâ”€â”€ qdrant_adapter.py             # Qdrant implementation
â”‚   â”œâ”€â”€ pgvector_adapter.py           # pgvector implementation
â”‚   â”œâ”€â”€ weaviate_adapter.py           # Weaviate implementation
â”‚   â”œâ”€â”€ milvus_adapter.py             # Milvus implementation
â”‚   â””â”€â”€ opensearch_adapter.py         # OpenSearch implementation (score fix applied)
â”œâ”€â”€ embeddings/
â”‚   â””â”€â”€ embedding_generator.py        # Sentence-transformers, OpenAI
â”œâ”€â”€ parsers/
â”‚   â””â”€â”€ document_parser.py            # TXT, PDF, DOCX parsing
â”œâ”€â”€ monitoring/
â”‚   â””â”€â”€ resource_monitor.py           # CPU, memory tracking
â””â”€â”€ utils/
    â””â”€â”€ chunking.py                   # Fixed, sentence, paragraph strategies

Data/test_corpus/
â”œâ”€â”€ documents/                        # 20 climate science docs
â””â”€â”€ test_cases.json                   # 10 test queries with ground truth

results/
â”œâ”€â”€ *_experiment_001/                 # Per-database results
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ results.json
â”‚   â””â”€â”€ performance_quality.png
â””â”€â”€ all_databases_comparison.png      # Cross-database comparison
```

---

## ğŸš€ How to Run

### Query Benchmark
```bash
# Start Qdrant
docker-compose up -d qdrant

# Run benchmark
source venv/bin/activate
python Scripts/run_qdrant_benchmark.py

# Results in: results/qdrant_experiment_001/
```

### Ingestion Benchmark
```bash
# Start Qdrant
docker-compose up -d qdrant

# Run benchmark
source venv/bin/activate
python Scripts/run_qdrant_ingestion_benchmark.py

# Results in: results/qdrant_ingestion_experiment_001/
```

---

## ğŸ“ Critical Next Step

### ğŸ”´ Priority #1: Dataset Integration

**Status**: All 7 databases are working with Climate Science dataset. **Next critical task is integrating the curated dataset from Google Drive**.

**Action Items**:
1. Download curated dataset from Google Drive
2. Update data loading scripts for new format
3. Verify compatibility with all 7 adapters
4. Re-run all benchmarks with new dataset
5. Update documentation with new results

## ğŸ“ Phase 3: Advanced Features & Research-Grade Extensions

### Enhanced Quality Metrics

1. **Precision@K, Recall@K**
   - Use existing ground truth labels in test_cases.json
   - Calculate for each database and Top-K value
   - Add to comparison plots

2. **NDCG & MRR**
   - Implement Normalized Discounted Cumulative Gain
   - Implement Mean Reciprocal Rank
   - Compare across databases

3. **LLM-as-Judge**
   - Generate answers from retrieved chunks
   - Use LLM to rate answer quality
   - Compare to ground_truth_answer

### Performance Enhancements

4. **Statistical Rigor**
   - Multiple benchmark runs (n=5 or 10)
   - Calculate mean Â± standard deviation
   - Add error bars to plots
   - Statistical significance testing

5. **Concurrent Query Testing**
   - Multi-threaded benchmark implementation
   - Test realistic query patterns
   - Measure under load

6. **Memory Profiling**
   - Track RAM usage during ingestion
   - Track RAM usage during queries
   - Add memory metrics to results

### Scale Testing

7. **Expand Test Corpus**
   - Scale to 100-1000 documents
   - Test with different domains
   - Multiple embedding models

8. **Production Scenarios**
   - Multi-user simulation
   - Cache performance analysis
   - Cost analysis (compute, memory)

---

## ğŸ“ Design Decisions & Rationale

### Why Semantic Similarity First?

**User requested**: "do 1 first then contributors can broaden later to reach research-grade"

**We chose semantic similarity because**:
1. âœ… **Speed to data**: Fully automated, no manual labeling
2. âœ… **Objective**: Reproducible across runs
3. âœ… **Meaningful**: Measures actual semantic relevance
4. âœ… **Extensible**: Foundation for more sophisticated metrics
5. âœ… **Standard practice**: Used in IR research (BEIR, MS MARCO)

**Alternative approaches (for contributors to add later)**:
- Precision@K requires manual relevance labels
- LLM-as-judge requires API costs and time
- Answer generation adds complexity

### Why Minimal Ingestion Configurations?

**User feedback**: "emphasis should be on end-to-end proof of concept. So, few test configurations at first. Speed to data is important."

**Configuration**:
- 3 chunk sizes Ã— 2 batch sizes Ã— 2 doc counts Ã— 1 run = 12 experiments
- Completes in <1 minute
- Provides enough data to see trends
- Easy to expand later

**Expandable to**:
- 5 chunk sizes Ã— 4 batch sizes Ã— 4 doc counts Ã— 5 runs = 400 experiments
- For comprehensive analysis when needed

---

## ğŸ”¬ Experimental Setup

### Embedding Model
- **Model**: sentence-transformers/all-MiniLM-L6-v2
- **Dimension**: 384
- **Why**: Fast, good quality, widely used benchmark model

### Test Corpus
- **Size**: 20 documents (~78KB total)
- **Domain**: Climate science (ice-albedo, greenhouse gases, ocean circulation, atmospheric physics)
- **Characteristics**: Well-structured scientific text, good for semantic search

### Test Queries
- **Count**: 10 queries
- **Design**: Each has ground truth answer and relevant document IDs
- **Purpose**: Enables future Precision@K and answer quality evaluation

### Database Configuration
- **Qdrant**: Docker container, localhost:6333
- **Distance metric**: Cosine similarity
- **Collection**: Recreated for each benchmark run (clean state)

---

## ğŸ“Š Visualization Outputs

### Query Benchmark: `performance_quality.png`
4-panel plot:
1. **Query Latency vs Top-K**: Shows latency scaling
2. **Throughput vs Top-K**: Shows QPS degradation with larger K
3. **Retrieval Quality**: Semantic similarity scores across K
4. **Quality-Speed Tradeoff**: Scatter plot for optimal K selection

### Ingestion Benchmark: 3 plots
1. **`ingestion_performance.png`**: 4-panel showing time, throughput, phase breakdown, batch impact
2. **`scaling_performance.png`**: Document count scaling analysis
3. **`ingestion_heatmap.png`**: Chunk size Ã— batch size heatmap

All plots:
- 300 DPI (publication ready)
- Professional styling
- Clear labels and legends

---

## ğŸ¤ For Contributors

### Adding a New Database

See `CONTRIBUTOR_GUIDE.md` for detailed instructions.

**Quick steps**:
1. Copy `src/vector_dbs/qdrant_adapter.py` as template
2. Implement the 5 required methods: connect, disconnect, create_collection, insert_chunks, query
3. Copy `Scripts/run_qdrant_benchmark.py` and adapt configuration
4. Update Docker Compose if needed
5. Test with test corpus
6. Submit PR

### Extending Quality Metrics

**Add Precision@K**:
```python
# In benchmark script, after query:
relevant_doc_ids = set(tc['relevant_doc_ids'])
retrieved_doc_ids = get_doc_ids_from_chunks(result_ids)
precision = len(relevant_doc_ids & retrieved_doc_ids) / len(retrieved_doc_ids)
```

**Add LLM-as-judge**:
```python
# Generate answer from chunks
answer = generate_answer(retrieved_chunks, query)
# Rate with LLM
score = llm_judge(query, answer, ground_truth)
```

---

## ğŸ› Known Issues & Limitations

### Current Limitations
1. **Small corpus**: Only 20 documents (fine for proof-of-concept)
2. **Single embedding model**: Only tested with MiniLM-L6-v2
3. **No concurrent queries**: Single-threaded benchmark
4. **No memory profiling**: Only measures time

### Not Bugs, Just Scope
- Results directory in .gitignore (intentional - regenerable data)
- No multi-user simulation (future enhancement)
- No cost analysis (future enhancement)

---

## ğŸ“š Related Documentation

- `IMPLEMENTATION_PLAN.md`: Overall project roadmap (v2.0 - iterative approach)
- `CONTRIBUTOR_GUIDE.md`: How to add new databases
- `START_HERE.md`: Quick start guide
- `APPROACH_COMPARISON.md`: Why iterative vs waterfall

---

## ğŸ’¡ Key Takeaways

### What Makes This a Good Foundation

1. **Working end-to-end example**: Not just design docs, but running code with real results
2. **Speed to data**: <1 minute ingestion benchmark, <30 seconds query benchmark
3. **Automated quality**: No manual work needed for basic quality assessment
4. **Extensible design**: Easy to add databases, metrics, configurations
5. **Production-ready code**: Error handling, logging, documentation
6. **Publication-ready outputs**: High-DPI plots, JSON results, CSV exports

### Success Criteria Met âœ…

- [x] Get experimental data quickly (speed-to-data)
- [x] Measure both performance AND quality
- [x] Create framework for contributors to expand
- [x] Generate publication-ready visualizations
- [x] Minimal manual effort required

---

## ğŸ”„ Resume Points

When returning to this project, recommended next steps:

1. **ğŸ”´ CRITICAL: Integrate curated dataset from Google Drive**
2. **Add error bars** to plots (run benchmarks multiple times, calculate std dev)
3. **Automation script** for running entire test suite unattended
4. **Add Precision@K, Recall@K** metrics using existing test_cases.json labels
5. **Begin manuscript writing** with collected data
6. **Scale test corpus** to 100+ documents

**Current State**:
- âœ… All 7 databases implemented and verified
- âœ… Similarity calculations validated ([BENCHMARK_VERIFICATION.md](BENCHMARK_VERIFICATION.md))
- âœ… Bug fixes applied (FAISS L2 distance, OpenSearch score normalization)
- â³ Ready for curated dataset integration

---

**Git Status**: All changes committed and pushed to `main` branch
**Last Updates**:
- Fixed FAISS L2 distance conversion bug (commit f192068)
- Fixed OpenSearch score normalization bug (commit 0330624)
- Verified all 7 databases (BENCHMARK_VERIFICATION.md added)
